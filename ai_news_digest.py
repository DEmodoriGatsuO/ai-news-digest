import feedparser
import datetime
import time
import os
import json
import google.generativeai as genai
import logging
from pathlib import Path

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('ai_news_digest')

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from config import AI_FEEDS, KEYWORDS
except ImportError:
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    logger.info("config.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    AI_FEEDS = [
        # Google/Geminié–¢é€£
        "https://blog.google/technology/ai/rss/",
        "https://ai.googleblog.com/feeds/posts/default",
        
        # Microsofté–¢é€£
        "https://blogs.microsoft.com/ai/feed/",
        "https://techcommunity.microsoft.com/t5/microsoft-ai-blog/bg-p/ArtificialIntelligence/rss",
        
        # Anthropic/Claudeé–¢é€£
        "https://www.anthropic.com/blog/rss"
    ]
    
    KEYWORDS = ["ç”ŸæˆAI", "å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«", "LLM", "Claude", "Gemini", "GPT", "äººå·¥çŸ¥èƒ½", 
               "æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "AIå€«ç†", "AIã‚¬ãƒãƒŠãƒ³ã‚¹", "ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«", "éŸ³å£°èªè­˜",
               "ç”»åƒèªè­˜", "è‡ªç„¶è¨€èªå‡¦ç†", "NLP", "å¼·åŒ–å­¦ç¿’", "Anthropic", "Microsoft AI",
               "Google AI", "OpenAI", "transformers", "RLHF", "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"]

# APIã‚­ãƒ¼è¨­å®š
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
else:
    logger.error("GEMINI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    raise ValueError("GEMINI_API_KEYãŒå¿…è¦ã§ã™ã€‚GitHub Secretsã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")

def get_feed_entries(feed_url, hours_back=24):
    """æŒ‡å®šæ™‚é–“å†…ã®è¨˜äº‹ã‚’å–å¾—"""
    entries = []
    current_time = datetime.datetime.now()
    
    try:
        feed = feedparser.parse(feed_url)
        for entry in feed.entries:
            # å…¬é–‹æ—¥ã‚’è§£æï¼ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åãŒç•°ãªã‚‹å ´åˆã«å¯¾å¿œï¼‰
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                published = datetime.datetime.fromtimestamp(time.mktime(entry.published_parsed))
            elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
                published = datetime.datetime.fromtimestamp(time.mktime(entry.updated_parsed))
            else:
                # æ—¥ä»˜æƒ…å ±ãŒãªã„å ´åˆã¯ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ç”¨
                published = current_time
            
            time_diff = current_time - published
            
            # æŒ‡å®šæ™‚é–“å†…ã®è¨˜äº‹ã®ã¿æŠ½å‡º
            if time_diff.total_seconds() < hours_back * 3600:
                entries.append({
                    "title": entry.title,
                    "link": entry.link,
                    "published": published,
                    "summary": entry.summary if hasattr(entry, "summary") else "",
                    "source": feed.feed.title if hasattr(feed.feed, "title") else feed_url
                })
        logger.info(f"ãƒ•ã‚£ãƒ¼ãƒ‰ {feed_url} ã‹ã‚‰ {len(entries)} ä»¶ã®è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"ãƒ•ã‚£ãƒ¼ãƒ‰ {feed_url} ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    return entries

def summarize_with_gemini(text, max_tokens=100):
    """Geminiå…¬å¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦è¦ç´„ã‚’ç”Ÿæˆ"""
    try:
        # Geminiãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        # è¦ç´„ã®ç”Ÿæˆ
        prompt = f"ä»¥ä¸‹ã®AIé–¢é€£è¨˜äº‹ã‚’3-4æ–‡ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã¨å½±éŸ¿ã‚’å«ã‚ã¦ãã ã•ã„ã€‚\n\n{text}"
        
        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=0.2
            )
        )
        
        return response.text
    except Exception as e:
        logger.error(f"è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        return f"è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)[:100]}..."

def filter_by_keywords(entry, keywords=KEYWORDS):
    """é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    # ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦ç´„ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
    text = (entry["title"] + " " + entry["summary"]).lower()
    for keyword in keywords:
        if keyword.lower() in text:
            return True
    
    return False

def format_text_digest(entries, max_entries=10):
    """ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’å‡ºåŠ›"""
    digest = f"ğŸ¤– AIæœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ ğŸ¤–\n{datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\n\n"
    
    for i, entry in enumerate(entries[:max_entries], 1):
        digest += f"ã€{i}ã€‘{entry['title']} ({entry['source']})\n"
        digest += f"ğŸ”— {entry['link']}\n"
        digest += f"ğŸ“… {entry['published'].strftime('%Yå¹´%mæœˆ%dæ—¥')}\n"
        digest += f"ğŸ’¡ {entry['ai_summary']}\n\n"
    
    digest += f"---\nåˆè¨ˆ {len(entries)} ä»¶ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚"
    return digest

def format_markdown_digest(entries, max_entries=10):
    """GitHub Pagesç”¨ã®Markdownå½¢å¼ã§ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’å‡ºåŠ›"""
    today = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    
    digest = f"""---
layout: default
title: AIæœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ {today}
---

# AIæœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ
**æ›´æ–°æ—¥æ™‚: {datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}**

"""
    
    for i, entry in enumerate(entries[:max_entries], 1):
        digest += f"## {i}. {entry['title']}\n\n"
        digest += f"**ã‚½ãƒ¼ã‚¹**: {entry['source']}  \n"
        digest += f"**æ—¥ä»˜**: {entry['published'].strftime('%Yå¹´%mæœˆ%dæ—¥')}  \n"
        digest += f"**ãƒªãƒ³ã‚¯**: [{entry['link']}]({entry['link']})  \n\n"
        digest += f"{entry['ai_summary']}  \n\n"
        digest += "---\n\n"
    
    digest += f"*åˆè¨ˆ {len(entries)} ä»¶ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚*\n"
    return digest

def update_index_page(latest_file_path):
    """GitHub Pagesç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°"""
    index_path = Path('docs/index.md')
    archives_path = Path('docs/archives')
    
    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¸€è¦§ã‚’å–å¾—
    archives = []
    if archives_path.exists():
        archives = sorted([f for f in archives_path.glob('*.md')], reverse=True)
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’ç”Ÿæˆ
    index_content = """---
layout: default
title: AI News Digest
---

# AI News Digest

æœ€æ–°ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è‡ªå‹•ã§åé›†ãƒ»è¦ç´„ã—ãŸãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã§ã™ã€‚

## æœ€æ–°ã®ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ

"""
    
    # æœ€æ–°ã®ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã¸ã®ãƒªãƒ³ã‚¯
    latest_filename = latest_file_path.name
    latest_date = latest_filename.replace('digest_', '').replace('.md', '')
    index_content += f"- [{latest_date}ã®ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ](./archives/{latest_filename})\n\n"
    
    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒªã‚¹ãƒˆ
    index_content += "## ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–\n\n"
    for archive in archives[:10]:  # æœ€æ–°10ä»¶ã®ã¿è¡¨ç¤º
        archive_date = archive.name.replace('digest_', '').replace('.md', '')
        index_content += f"- [{archive_date}](./archives/{archive.name})\n"
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜
    if not index_path.parent.exists():
        index_path.parent.mkdir(parents=True)
    
    index_path.write_text(index_content, encoding='utf-8')
    logger.info(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {index_path}")

def send_to_slack(digest, webhook_url):
    """Slackã«ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’é€ä¿¡"""
    if not webhook_url:
        logger.warning("SLACK_WEBHOOK_URLãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€Slacké€šçŸ¥ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return False
    
    try:
        import requests
        
        # æ–‡å­—æ•°åˆ¶é™ã«å¯¾å¿œã™ã‚‹ãŸã‚è¦ç´„
        short_digest = digest.split("\n\n")[0] + "\n\n"  # ã‚¿ã‚¤ãƒˆãƒ«éƒ¨åˆ†ã®ã¿
        short_digest += "æ–°ã—ã„AIãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚è©³ç´°ã¯GitHub Pagesã‚’ã”è¦§ãã ã•ã„ã€‚"
        
        payload = {
            "text": short_digest,
            "blocks": [
                {
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": short_digest
                    }
                },
                {
                    "type": "actions",
                    "elements": [
                        {
                            "type": "button",
                            "text": {
                                "type": "plain_text",
                                "text": "è©³ç´°ã‚’è¦‹ã‚‹"
                            },
                            "url": "https://[username].github.io/ai-news-digest/"  # GitHubãƒ¦ãƒ¼ã‚¶ãƒ¼åã¨ãƒªãƒã‚¸ãƒˆãƒªåã«å¿œã˜ã¦å¤‰æ›´
                        }
                    ]
                }
            ]
        }
        
        response = requests.post(webhook_url, json=payload)
        if response.status_code == 200:
            logger.info("Slackã¸ã®é€šçŸ¥ã«æˆåŠŸã—ã¾ã—ãŸ")
            return True
        else:
            logger.error(f"Slackã¸ã®é€šçŸ¥ã«å¤±æ•—ã—ã¾ã—ãŸ: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
            return False
    except Exception as e:
        logger.error(f"Slacké€šçŸ¥ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False

def main():
    today_str = datetime.datetime.now().strftime('%Y%m%d')
    all_entries = []
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™
    docs_path = Path('docs')
    archives_path = Path('docs/archives')
    
    if not docs_path.exists():
        docs_path.mkdir(parents=True)
    
    if not archives_path.exists():
        archives_path.mkdir(parents=True)
    
    # å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’åé›†
    for feed_url in AI_FEEDS:
        logger.info(f"ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’è§£æä¸­: {feed_url}")
        entries = get_feed_entries(feed_url)
        all_entries.extend(entries)
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_entries = [entry for entry in all_entries if filter_by_keywords(entry)]
    logger.info(f"åˆè¨ˆ {len(filtered_entries)} ä»¶ã®é–¢é€£è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
    
    # å…¬é–‹æ—¥ã§ä¸¦ã¹æ›¿ãˆ
    filtered_entries.sort(key=lambda x: x["published"], reverse=True)
    
    # è¨˜äº‹ãŒãªã„å ´åˆã®å‡¦ç†
    if not filtered_entries:
        logger.warning("é–¢é€£è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # è¦ç´„ã‚’ç”Ÿæˆ
    for entry in filtered_entries:
        content = f"ã‚¿ã‚¤ãƒˆãƒ«: {entry['title']}\næœ¬æ–‡: {entry['summary']}"
        entry["ai_summary"] = summarize_with_gemini(content)
        time.sleep(1)
    
    # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã®ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’ç”Ÿæˆ
    text_digest = format_text_digest(filtered_entries)
    
    # Markdownå½¢å¼ã®ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’ç”Ÿæˆ
    md_digest = format_markdown_digest(filtered_entries)
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    txt_path = Path(f"ai_digest_{today_str}.txt")
    txt_path.write_text(text_digest, encoding='utf-8')
    logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {txt_path}")
    
    # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ (GitHub Pagesç”¨)
    md_filename = f"digest_{today_str}.md"
    md_path = archives_path / md_filename
    md_path.write_text(md_digest, encoding='utf-8')
    logger.info(f"Markdownãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {md_path}")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°
    update_index_page(md_path)

if __name__ == "__main__":
    main()