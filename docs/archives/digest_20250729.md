---
layout: default
title: AI最新ニュースダイジェスト 2025年07月29日
---

# AI最新ニュースダイジェスト
**更新日時: 2025年07月29日 13:03**

## 1. Agent WARPP: Workflow Adherence via Runtime Parallel Personalization

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月29日  
**リンク**: [https://arxiv.org/abs/2507.19543](https://arxiv.org/abs/2507.19543)  

この論文は、大規模言語モデル（LLM）を用いたタスク指向対話システムにおける、複雑なワークフローの実行能力を向上させる新しいフレームワーク「WARPP」を提案しています。WARPPは、マルチエージェントアーキテクチャとランタイムパーソナライゼーションを組み合わせ、ユーザー属性に基づいて実行パスを動的に調整することで、LLMの推論負荷を軽減し、ツール選択の精度を高めます。評価結果は、WARPPが  

---

## 2. DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月29日  
**リンク**: [https://arxiv.org/abs/2507.19608](https://arxiv.org/abs/2507.19608)  

DeltaLLMは、大規模言語モデル (LLM) をエッジデバイスで効率的に推論するための、トレーニング不要のフレームワークです。このフレームワークは、注意パターンにおける時間的スパース性を活用し、精度を維持しながら計算量を削減します。DeltaLLMは、ローカルコンテキストウィンドウ内でのフルアテンションと、それ以外のデルタ近似を組み合わせたハイブリッドアテンションメカニズムを採用し、BitNetやLlama3などのモデル  

---

## 3. Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月29日  
**リンク**: [https://arxiv.org/abs/2507.19672](https://arxiv.org/abs/2507.19672)  

この論文は、大規模言語モデル（LLM）の安全性と人間との整合性に関する包括的な調査です。LLMの社会への影響が大きくなるにつれて、人間の価値観との整合性が重要になっており、論文では様々なアライメント技術、トレーニング方法、および評価フレームワークを分析しています。特に、Direct Preference Optimization (DPO)などの最新技術に焦点を当て、品質と効率のバランス、および報酬の誤指定や堅牢性などの課題を  

---

## 4. The wall confronting large language models

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月29日  
**リンク**: [https://arxiv.org/abs/2507.19703](https://arxiv.org/abs/2507.19703)  

この論文は、大規模言語モデル（LLM）の予測の不確実性を改善する能力が、スケーリング法則によって深刻に制限されていると主張しています。LLMの学習能力の根源である非ガウス分布の生成が、エラーの蓄積や情報崩壊を引き起こし、AIの劣化につながる可能性があると指摘しています。論文は、データのサイズが増加するにつれて、誤った相関関係が急増することも強調しています。この研究  

---

## 5. HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月29日  
**リンク**: [https://arxiv.org/abs/2507.19726](https://arxiv.org/abs/2507.19726)  

この論文は、精密医療のために、ハイパーグラフベースの知識グラフ（KG）に患者の電子健康記録（EHR）を統合するフレームワーク「HypKG」を提案しています。HypKGは、既存の知識グラフと患者の文脈を組み合わせ、より正確な医療予測を可能にするコンテキスト化された知識表現を生成します。実験結果は、HypKGが医療予測タスクにおいて大幅な改善を示し、知識グラフの品質と実用  

---

## 6. Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月29日  
**リンク**: [https://arxiv.org/abs/2507.19749](https://arxiv.org/abs/2507.19749)  

この論文は、大規模言語モデル（LLM）が非単調推論の強力な手法であるAnswer Set Programming（ASP）問題を解決できるかを評価しています。研究では、ASPに特化した新しいベンチマークASPBenchを導入し、14の最先端LLMを評価しました。結果として、LLMはASPの基本的なタスクにはある程度対応できるものの、ASP解決の核心である答え集合の計算には苦戦しており、LLMの現在の限界  

---

## 7. Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月29日  
**リンク**: [https://arxiv.org/abs/2507.19973](https://arxiv.org/abs/2507.19973)  

この研究は、大規模言語モデル（LLM）を微調整して、MRI/CTレポートから膵嚢胞性病変（PCL）の特徴を抽出し、リスク分類を自動化することを目指しています。LLaMAとDeepSeekという2つのオープンソースLLMを、GPT-4oが生成したデータで微調整した結果、特徴抽出精度とリスク分類のパフォーマンスが大幅に向上し、GPT-4oに匹敵するレベルに達しました  

---

## 8. Matching Game Preferences Through Dialogical Large Language Models: A Perspective

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月29日  
**リンク**: [https://arxiv.org/abs/2507.20000](https://arxiv.org/abs/2507.20000)  

この論文は、大規模言語モデル（LLM）とGRAPHYPのネットワークシステムを組み合わせ、人間の会話と嗜好をより深く理解する「対話型インテリジェンス」の可能性を探求しています。提案されている「対話型LLM（D-LLM）」フレームワークは、複数のユーザーが構造化された会話を通じて異なる嗜好を共有し、AIの意思決定に個々のユーザーの嗜好を直接組み込むことを目指しています  

---

## 9. PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月29日  
**リンク**: [https://arxiv.org/abs/2507.20067](https://arxiv.org/abs/2507.20067)  

この論文は、大規模言語モデル（LLM）の推論時に、追加の学習なしでユーザーの好みに合わせた出力を生成する新しい手法「PITA」を提案しています。PITAは、報酬モデルに依存せず、小さなガイドモデルを用いてトークン生成を直接調整することで、計算コストを削減し、不安定な報酬モデルの学習を回避します。PITAは、好みに基づくガイドポリシーを学習し、トークン確率を調整することで、数学  

---

## 10. The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月29日  
**リンク**: [https://arxiv.org/abs/2507.20150](https://arxiv.org/abs/2507.20150)  

この論文は、大規模言語モデル（LLM）における報酬関数と最適ポリシーの関係を数学的に分析し、LLMの不安定性の根本原因を解明しています。研究では、報酬の不完全性やノイズが、誤った推論や指示不履行といった問題を引き起こす可能性を示唆しています。この理論的枠組みは、LLMの安全性と信頼性を高めるための、より安定したポリシー設計に貢献し、AIシステムの開発に重要な影響  

---

*合計 142 件のAI関連ニュースが見つかりました。*
