---
layout: default
title: AI最新ニュースダイジェスト 2025年12月24日
---

# AI最新ニュースダイジェスト
**更新日時: 2025年12月24日 12:57**

## 1. PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年12月24日  
**リンク**: [https://arxiv.org/abs/2512.19799](https://arxiv.org/abs/2512.19799)  

この論文は、大規模言語モデル（LLM）を活用して、理論物理学と計算物理学の研究を支援する自律型AIエージェント「PhysMaster」を提案しています。PhysMasterは、抽象的な推論と数値計算を組み合わせ、文献検索や知識ベースを活用することで、研究の加速、自動化、そして自律的な発見を可能にします。高エネルギー理論、凝縮系理論、天体物理学などの分野で、研究期間を大幅に  

---

## 2. Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年12月24日  
**リンク**: [https://arxiv.org/abs/2512.19937](https://arxiv.org/abs/2512.19937)  

この論文は、大規模言語モデル（LLM）の性格特性を効率的に操作し、人間の行動を模倣する方法を提案しています。研究者たちは、補間デコーディングを用いて、性格特性の各次元を対立するプロンプトのペアとして表現し、補間パラメータで行動をシミュレートします。この手法により、LLMは経済ゲームで人間の意思決定を再現し、個々の人間の行動を模倣することが可能になります。この研究  

---

## 3. S$^3$IT: A Benchmark for Spatially Situated Social Intelligence Test

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年12月24日  
**リンク**: [https://arxiv.org/abs/2512.19992](https://arxiv.org/abs/2512.19992)  

この論文は、空間的に配置された社会知能を評価するための新しいベンチマーク「S$^3$IT」を紹介しています。S$^3$ITは、言語モデル（LLM）駆動のNPCが持つ多様な個性、好み、人間関係を考慮し、3D環境で座席を配置する課題を提供します。このベンチマークは、LLMが現実的な環境における物理的制約と社会的規範を統合する能力を評価することを目的として  

---

## 4. Scaling Reinforcement Learning for Content Moderation with Large Language Models

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年12月24日  
**リンク**: [https://arxiv.org/abs/2512.20061](https://arxiv.org/abs/2512.20061)  

この論文は、大規模言語モデル（LLM）を用いたコンテンツモデレーションにおける強化学習（RL）の拡張性を調査しています。研究では、RLがデータ効率を大幅に向上させ、複雑なポリシーに基づいた判断が必要なタスクで優れた性能を発揮することを示しています。RLは、トレーニングデータ、ロールアウト、最適化ステップの増加に伴い、徐々に性能が向上し、最終的に飽和する「シグモイド型」のスケール  

---

## 5. Reason2Decide: Rationale-Driven Multi-Task Learning

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年12月24日  
**リンク**: [https://arxiv.org/abs/2512.20074](https://arxiv.org/abs/2512.20074)  

この論文は、臨床意思決定支援システム向けに、予測精度と説明可能性を両立させる新しいAIフレームワーク「Reason2Decide」を提案しています。Reason2Decideは、自己正当化における課題（暴露バイアスなど）に対処するため、2段階のトレーニングを採用し、予測と説明の整合性を高めています。実験結果では、Reason2Decideが既存手法を上回り、LLM生成の正当化を事前学習に  

---

## 6. Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年12月24日  
**リンク**: [https://arxiv.org/abs/2512.20082](https://arxiv.org/abs/2512.20082)  

この論文は、インドの株式市場における投資判断を支援するため、LLM、RAG、強化学習を組み合わせた適応型金融センチメント分析フレームワークを提案しています。具体的には、LLaMA 3.2 3BモデルをSentiFinデータセットで微調整し、RAGパイプラインで文脈情報を動的に選択し、強化学習エージェントが市場のフィードバックに基づいてソースの信頼性を調整します。2  

---

## 7. MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年12月24日  
**リンク**: [https://arxiv.org/abs/2512.20135](https://arxiv.org/abs/2512.20135)  

この論文は、分子編集と特性最適化をエージェント型強化学習（RL）問題として捉えた新しいフレームワーク「MolAct」を提案しています。MolActは、LLMエージェントが化学ツールを活用して分子の妥当性、特性評価、類似性を制御しながら、編集と最適化を反復的に行うことを学習します。実験結果は、MolActが既存のベースラインを上回り、分子設計を多段階のツール活用  

---

## 8. Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年12月24日  
**リンク**: [https://arxiv.org/abs/2512.20140](https://arxiv.org/abs/2512.20140)  

この論文は、大規模言語モデル（LLM）をゼロショット時系列予測に利用する際の課題に取り組み、特に、LLMのパラメータを一切変更しない「オフザシェルフ」モデルの性能向上に焦点を当てています。著者は、時系列データにノイズを注入することで、LLMがデータの表面的な数値ではなく、より堅牢な時間的パターンに基づいて予測を行うように促す手法を提案しています。この手法は、新しい時系列データセット  

---

## 9. Concept Generalization in Humans and Large Language Models: Insights from the Number Game

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年12月24日  
**リンク**: [https://arxiv.org/abs/2512.20162](https://arxiv.org/abs/2512.20162)  

この論文は、人間と大規模言語モデル（LLM）が概念を一般化する能力を、数字ゲームという概念推論タスクを通して比較しています。研究の結果、人間はルールベースと類似性ベースの両方の概念を柔軟に推論する一方、LLMは数学的ルールに依存する傾向があることが判明しました。また、人間は少数の例からでも一般化できるのに対し、LLMはより多くのサンプルを必要としました。これらの違い  

---

## 10. Offline Safe Policy Optimization From Heterogeneous Feedback

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年12月24日  
**リンク**: [https://arxiv.org/abs/2512.20173](https://arxiv.org/abs/2512.20173)  

この論文は、オフラインの人間フィードバックから安全な強化学習を行う新しい手法「PreSa」を提案しています。PreSaは、報酬と安全性の両方に関する人間の選好を直接学習し、制約付き最適化問題を通じて安全なポリシーを直接学習します。従来の報酬とコストモデルを学習するアプローチと比較して、PreSaはエラーの蓄積を回避し、連続制御タスクにおいて高い報酬と安全性を両立したポリシー  

---

*合計 80 件のAI関連ニュースが見つかりました。*
