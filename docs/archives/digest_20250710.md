---
layout: default
title: AI最新ニュースダイジェスト 2025年07月10日
---

# AI最新ニュースダイジェスト
**更新日時: 2025年07月10日 12:56**

## 1. Representing Prompting Patterns with PDL: Compliance Agent Case Study

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月10日  
**リンク**: [https://arxiv.org/abs/2507.06396](https://arxiv.org/abs/2507.06396)  

この論文は、大規模言語モデル（LLM）のプロンプトエンジニアリングの複雑さに対処するため、新しいプロンプト表現方法であるPrompt Declaration Language（PDL）を紹介しています。PDLは、LLM呼び出しとルールベースのコード、外部ツールを組み合わせることで、プロンプトの構成を捉え、手動および自動でのプロンプト調整を可能にします。コンプライアンスエージェントのケーススタディでは、PDL  

---

## 2. The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月10日  
**リンク**: [https://arxiv.org/abs/2507.06993](https://arxiv.org/abs/2507.06993)  

この論文は、大規模言語モデル（LLM）を活用して、旅行計画、ナビゲーション、そして状況変化への動的適応を強化するユーザー中心の地理体験フレームワークを提案しています。従来のシステムが抱える課題を解決するため、旅行計画エージェント、目的地アシスタントエージェント、ローカル発見エージェントの3つの協調エージェントを開発し、クエリ解釈、ナビゲーション精度、そして予期せぬ  

---

## 3. First Return, Entropy-Eliciting Explore

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月10日  
**リンク**: [https://arxiv.org/abs/2507.07017](https://arxiv.org/abs/2507.07017)  

この論文は、大規模言語モデル（LLM）の推論能力を向上させるための新しい探索フレームワーク「FR3E」を提案しています。FR3Eは、不安定な探索というRLVRの課題を解決するため、推論軌跡における不確実性の高い決定ポイントを特定し、中間的なフィードバックを構築します。これにより、より安定した学習、より長く一貫性のある応答、そして完全な正解軌跡の増加を実現し  

---

## 4. False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月10日  
**リンク**: [https://arxiv.org/abs/2507.06252](https://arxiv.org/abs/2507.06252)  

この論文は、大規模言語モデル（LLM）を活用したテキストベースのサイバー脅威インテリジェンス（CTI）システムに対する敵対的攻撃の脆弱性を調査しています。研究では、偽のテキスト生成技術を用いて、CTIパイプラインの分類器を欺き、システムのパフォーマンスを低下させるエバジョン攻撃に焦点を当てています。これらの攻撃は、CTIシステムが誤った情報を収集し、誤ったアラートを引き起こす可能性  

---

## 5. Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月10日  
**リンク**: [https://arxiv.org/abs/2507.06256](https://arxiv.org/abs/2507.06256)  

この論文は、音声ベースのLLM（Qwen2-Audioなど）が、攻撃者によって現実世界で操作される脆弱性があることを示しています。攻撃者は、LLMを特定の行動（ウェイクワードへの応答や有害な行動のトリガーなど）を誘発するような、ステルス性の高い音声ノイズを生成できます。さらに、攻撃的な背景ノイズは、ユーザーとのインタラクション中に応答品質を低下させ、他のユーザーにも影響を与える  

---

## 6. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月10日  
**リンク**: [https://arxiv.org/abs/2507.06261](https://arxiv.org/abs/2507.06261)  

Googleは、高度な推論、マルチモーダル機能、長文コンテキスト処理、次世代エージェント能力を備えたGemini 2.5モデルファミリーを発表しました。最上位モデルであるGemini 2.5 Proは、最先端のコーディングと推論ベンチマークで優れた性能を発揮し、最大3時間の動画コンテンツを処理できます。このモデルは、長文コンテキスト、マルチモーダル、推論能力を組み合わせることで、新たな  

---

## 7. Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月10日  
**リンク**: [https://arxiv.org/abs/2507.06274](https://arxiv.org/abs/2507.06274)  

この論文は、大規模言語モデル（LLM）の不正利用を防ぐためのウォーターマーク技術の脆弱性に対処しています。従来のウォーターマークは、除去や偽装攻撃に対してトレードオフの関係にありましたが、この研究では「等価テクスチャキー」という新しいメカニズムを導入し、この問題を解決しました。提案されたSEEKウォーターマークスキームは、除去攻撃に対する耐性を高めつつ、偽装攻撃に対する堅牢性を  

---

## 8. The Prompt War: How AI Decides on a Military Intervention

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月10日  
**リンク**: [https://arxiv.org/abs/2507.06277](https://arxiv.org/abs/2507.06277)  

この論文は、AIが軍事介入を決定する際にどのような要因を重視するのかを分析しています。実験を通して、AIは国内支持率と勝利の可能性を最も重視し、国際的な非難や犠牲者数などのコストはそれらの半分程度の影響力しかないことが明らかになりました。興味深いことに、OpenAI GPT、Anthropic Claude、Google Geminiといった異なるAIモデル間で、意思決定パターンに一貫性が見られました。この研究は、AIが軍  

---

## 9. The bitter lesson of misuse detection

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月10日  
**リンク**: [https://arxiv.org/abs/2507.06282](https://arxiv.org/abs/2507.06282)  

この論文は、LLM（大規模言語モデル）の不正利用検出における既存の監視システムの限界を浮き彫りにしています。新しいベンチマークBELLSを用いて、市販の監視システムが多様な攻撃に対して脆弱であり、特に新しい攻撃手法や直接的な有害な質問に対して検出率が低いことを明らかにしました。驚くべきことに、汎用LLMの方がこれらの監視システムよりも優れたパフォーマンスを示し、LLMの一般的な能力が不正利用検出に不可欠であることが  

---

## 10. Humans overrely on overconfident language models, across languages

**ソース**: cs.AI updates on arXiv.org  
**日付**: 2025年07月10日  
**リンク**: [https://arxiv.org/abs/2507.06306](https://arxiv.org/abs/2507.06306)  

この論文は、多言語対応のLLMが、自信過剰な回答を生成し、ユーザーが過度に依存するリスクが高いことを示しています。研究では、5つの言語でLLMの回答における確信度マーカーの分布を分析し、言語によって異なる過信傾向を発見しました。その結果、ユーザーはどの言語でも自信のある回答に強く依存し、特に日本語では不確実性を示す表現にも依存する傾向があることが分かりました。  

---

*合計 82 件のAI関連ニュースが見つかりました。*
